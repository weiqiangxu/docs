---
title: 如何保证不重复消费又不丢失数据
tags:
  - kafka原理
categories:
  - kafka
date: 2023-04-08 06:40:12
index_img: /images/bg/computer.jpeg
hide: true
---

### 问题罗列

1. Producer发消息至Broker的时候，就有可能会丢消息 | producer -> broker
2. 把消息发送到Broker以后，也有可能丢消息 | broker->磁盘 | broker->broker之间同步
3. client消费数据时候丢失数据或者重复消费 | 常见场景

### 1和2的问题如何解决
```
callback的api
acks
retries
factor
```

### 3的问题如何解决

1. 不想消费者丢失数据那就手动提交，等消费逻辑正常跑完手动提交offset (auto commit = false)
2. 不想重复消费呢（导致重复消费的无非是client已经消费了数据，但是offset没来得及提交）

```
场景：

a. pull 500条数据消费(offset=10~510)，在5min没消费完kafka认为consumer已经炸了，触发rebalance把offset=10~510的数据给其他消费者，重复消费
b. 消费了数据以后，提交offset之前，kill了线程（宕机、重启）
```
```
解决方案：消费逻辑的幂等
```

### 参考文章
[kafka 如何保证不重复消费又不丢失数据](https://www.zhihu.com/question/483747691/answer/2392949203)
[图解Kafka的架构和消费原理](https://zhuanlan.zhihu.com/p/442468709)